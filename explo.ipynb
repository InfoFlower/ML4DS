{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de Machine Learning for data sciences\n",
    "Utilisation des données internet sur les articles publiés sur https://news.bitcoin.com/ et les données marché pour prédire la montée ou la descente du BitCoin.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des packages\n",
    "Les fonctions créées pour le projet seront expliquée lors de l'utilisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #pandas\n",
    "import web_knwoledge.api_bitcoin as bitcoin_api #data making\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut lancer web_knowledge.scrapper sur l'invite de commande (ou utiliser les données déjà récoltées). \n",
    "        ==> dossier wayback_dowloads\n",
    "> python web_knowledge/scrapper.py 'https://news.bitcoin.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération des données de binance :\n",
    "reload(bitcoin_api)\n",
    "data = bitcoin_api.get_data('1w') # On est limité à un interval de 1 semaine pour s'accorder aux données web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données disponibles grâce à l'appel API binance :\n",
    "- Open time: The timestamp (in milliseconds) indicating when the candlestick period starts.\n",
    "- Open: The opening price of the asset for the candlestick period.\n",
    "- High: The highest price of the asset during the candlestick period.\n",
    "- Low: The lowest price of the asset during the candlestick period.\n",
    "- Close: The closing price of the asset for the candlestick period.\n",
    "- Volume: The trading volume of the asset for the candlestick period.\n",
    "- Close time: The timestamp (in milliseconds) indicating when the candlestick period ends.\n",
    "- Quote asset volume: The trading volume of the quote asset for the candlestick period.\n",
    "- Number of trades: The number of trades that occurred during the candlestick period.\n",
    "- Taker buy base asset volume: The volume of the base asset involved in taker trades during the candlestick period.\n",
    "- Taker buy quote asset volume: The volume of the quote asset involved in taker trades during the candlestick period.\n",
    "- Ignore: This column is typically ignored and contains no relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Open time\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"Close time\",\"Quote asset volume\",\"Number of trades\",\"Taker buy base\",\"Taker buy quote\",\"Ignore\"]\n",
    "Binance_dataframe=pd.DataFrame(data,columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On va maintenant organiser nos données pour permettre la jointure avec les données web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_time=[datetime.strftime(datetime.fromtimestamp(i/1000),format='%Y%m%d') for i in Binance_dataframe[\"Open time\"]]\n",
    "Binance_dataframe[\"Open time\"]=open_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sauvegarde des données\n",
    "Binance_dataframe.to_csv(\"data/market/Binance_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On va maintenant convertir nos fichiers html en txt (formaté)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.from_html_to_txt import extract_content_from_html as ext_html\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"wayback_downloads\")\n",
    "log=''\n",
    "for i in files:\n",
    "    already_done = os.listdir(\"data/html/\")\n",
    "    if f'{i.split(\".\")[0][:8]}.txt' not in already_done:\n",
    "        try:\n",
    "            ext_html(\"wayback_downloads/\" + i, ['div', 'p', 'h6', 'h4'], f'data/html/{i.split(\".\")[0][:8]}.txt')\n",
    "        except Exception as e:\n",
    "            log+=f\"Failed to process {i}: {e}\\n\"\n",
    "            with open(\"log.txt\",\"w\") as f: f.write(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataframe pour ACM\n",
    "On utilise nos fichiers HTML formaté en txt pour compter les mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.text_processing import proc_make_csv as make_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on lemmatise notre texte pour qu'il ne contienne que les mots importants (pas générique) du texte.\n",
    "Réutilisation d'un script fait en Analyse textuelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=make_csv()\n",
    "word_count.index = [i[:8] for i in os.listdir(\"data/html/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.to_csv(\"data/word_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=pd.read_csv(\"data/word_count.csv\",index_col=0)\n",
    "word_count=word_count.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_13264\\4113805289.py:3: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  int(cache.iloc[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache=word_count.iloc[0].value_counts()\n",
    "cache=pd.DataFrame(data=cache.to_numpy().flatten(),index=cache.index.to_numpy().flatten(),columns=[\"count\"])\n",
    "int(cache.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_13264\\3963066350.py:7: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  elif int(cache.iloc[0])>len(word_count.columns)*0.5:\n"
     ]
    }
   ],
   "source": [
    "to_drop=[]\n",
    "for i in range(len(word_count.index)):\n",
    "    cache=word_count.iloc[i].value_counts()\n",
    "    cache=pd.DataFrame(data=cache.to_numpy().flatten(),index=cache.index.to_numpy().flatten(),columns=[\"count\"])\n",
    "    if 0 not in cache.index:\n",
    "        to_drop.append(word_count.index[i])\n",
    "    elif int(cache.iloc[0])>len(word_count.columns)*0.5:\n",
    "        to_drop.append(word_count.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> len(word_count.index)-len(to_drop)\n",
    ">>175\n",
    "\n",
    "On garde alors 175 mots sur les plus de 9000 existants ==> Analyse globale permettant clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.drop(to_drop,inplace=True)\n",
    "word_count.to_csv(\"data/word_count.csv\") #check point\n",
    "word_count=word_count.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On formate les données extraite des pages html pour n'avoir qu'un enregistrement par semaine pour s'accorder aux données de marché."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Binance_dataframe['Open time']:\n",
    "    borne_inf=int(datetime.strftime(datetime.strptime(Binance_dataframe[\"Open time\"][i],'%Y%m%d') - timedelta(days=7),format='%Y%m%d'))\n",
    "    borne_sup=int(datetime.strftime(datetime.strptime(Binance_dataframe[\"Open time\"][i],'%Y%m%d'),format='%Y%m%d'))\n",
    "    word_count=word_count.index[(word_count.index>=borne_inf) & (word_count.index<=borne_sup)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=pd.read_csv(\"data/word_count.csv\",index_col=0)\n",
    "word_count=word_count.T\n",
    "word_count.index=[int(i) for i in word_count.index]\n",
    "index_to_keep=[]\n",
    "for i in range(len(Binance_dataframe['Open time'])):\n",
    "    borne=int(datetime.strftime(datetime.strptime(Binance_dataframe[\"Open time\"][i],'%Y%m%d'),format='%Y%m%d'))\n",
    "    cache=max(word_count.index[(word_count.index<=borne)])\n",
    "    if cache not in index_to_keep:\n",
    "        index_to_keep.append(cache)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_mlds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
